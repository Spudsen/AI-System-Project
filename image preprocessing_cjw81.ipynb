{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocess images and output a dataset that has a directory structure that represents a binary classification - fracture or no fracture."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from PIL import Image, ImageEnhance, ImageChops, ImageOps\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os import path, makedirs, walk, listdir\n",
    "import shutil\n",
    "import cv2\n",
    "import glob\n",
    "from shutil import copyfile\n"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-23T08:33:52.919594Z",
     "start_time": "2024-03-23T08:33:52.917255Z"
    }
   },
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Give File Locations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-22T16:25:11.105585Z",
     "start_time": "2024-03-22T16:25:11.099140Z"
    }
   },
   "outputs": [],
   "source": [
    "#replace with own directoires\n",
    "training_directory = path.join('retinanet_data_format/train')\n",
    "testing_directory = path.join('retinanet_data_format/test')\n",
    "valid_directory = path.join('retinanet_data_format/valid')\n",
    "output_directory =path.join('retinanet_data_format')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split Images into entropy levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-22T16:25:33.253867Z",
     "start_time": "2024-03-22T16:25:16.633453Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not process retinanet_data_format/train/entropy_groups: [Errno 21] Is a directory: 'retinanet_data_format/train/entropy_groups'\n",
      "Could not process retinanet_data_format/train/_annotations.csv: cannot identify image file 'retinanet_data_format/train/_annotations.csv'\n",
      "Could not process retinanet_data_format/test/entropy_groups: [Errno 21] Is a directory: 'retinanet_data_format/test/entropy_groups'\n",
      "Could not process retinanet_data_format/test/_annotations.csv: cannot identify image file 'retinanet_data_format/test/_annotations.csv'\n",
      "Could not process retinanet_data_format/valid/entropy_groups: [Errno 21] Is a directory: 'retinanet_data_format/valid/entropy_groups'\n",
      "Could not process retinanet_data_format/valid/_annotations.csv: cannot identify image file 'retinanet_data_format/valid/_annotations.csv'\n"
     ]
    }
   ],
   "source": [
    "def calculate_entropy(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    grayscale_image = image.convert(\"L\")\n",
    "    image_array = np.array(grayscale_image)\n",
    "    histogram, _ = np.histogram(image_array, bins=256, range=(0, 255), density=True)\n",
    "    entropy = -np.sum(histogram * np.log2(histogram + 1e-10))  # adding epsilon to avoid log(0)\n",
    "    return entropy\n",
    "\n",
    "def split_images_into_entropy_groups(directory, entropy_bins, output_dir):\n",
    "    # Create a new directory for the entropy groups\n",
    "    entropy_groups_dir = path.join(output_dir, 'entropy_groups')\n",
    "    makedirs(entropy_groups_dir, exist_ok=True)\n",
    "\n",
    "    for i in range(len(entropy_bins) - 1):\n",
    "        # Create directories for the entropy groups\n",
    "        makedirs(path.join(entropy_groups_dir, f'entropy_group_{i}'), exist_ok=True)\n",
    "\n",
    "    for filename in listdir(directory):\n",
    "        file_path = path.join(directory, filename)\n",
    "        try:\n",
    "            entropy = calculate_entropy(file_path)\n",
    "            # Assign the image to an entropy group\n",
    "            for i in range(len(entropy_bins) - 1):\n",
    "                if entropy_bins[i] <= entropy < entropy_bins[i + 1]:\n",
    "                    # Copy the image to the corresponding directory\n",
    "                    shutil.copy(file_path, path.join(entropy_groups_dir, f'entropy_group_{i}'))\n",
    "                    break\n",
    "        except Exception as e:\n",
    "            print(f\"Could not process {file_path}: {e}\")\n",
    "\n",
    "#entropy bins\n",
    "entropy_bins = [0, 2, 3, 3.5, 4, float('inf')] \n",
    "\n",
    "# Run the function for each directory\n",
    "split_images_into_entropy_groups(training_directory, entropy_bins, training_directory)\n",
    "split_images_into_entropy_groups(testing_directory, entropy_bins, testing_directory)\n",
    "split_images_into_entropy_groups(valid_directory, entropy_bins, valid_directory)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sub-Split into Brightness Levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-22T16:26:25.441937Z",
     "start_time": "2024-03-22T16:26:20.707692Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_brightness(image):\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    return np.mean(hsv[:,:,2])\n",
    "\n",
    "def categorize_images(path):\n",
    "    categories = ['very_dark_brightness', 'dark_brightness', 'extra_low_brightness', \n",
    "                  'very_low_brightness', 'low_brightness', 'medium_brightness', 'high_brightness']\n",
    "\n",
    "    for filename in listdir(path):\n",
    "        if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "            image = cv2.imread(path.join(path, filename))\n",
    "            brightness = calculate_brightness(image)\n",
    "            if brightness < 5:\n",
    "                category = categories[0]\n",
    "            elif brightness < 10:\n",
    "                category = categories[1]\n",
    "            elif brightness < 21:\n",
    "                category = categories[2]\n",
    "            elif brightness < 42:\n",
    "                category = categories[3]\n",
    "            elif brightness < 85:\n",
    "                category = categories[4]\n",
    "            elif brightness < 170:\n",
    "                category = categories[5]\n",
    "            else:\n",
    "                category = categories[6]\n",
    "\n",
    "            new_dir = path.join(path, category)\n",
    "            if not path.exists(new_dir):\n",
    "                makedirs(new_dir)\n",
    "\n",
    "            shutil.copy(path.join(path, filename), path.join(new_dir, filename))\n",
    "\n",
    "\n",
    "# Apply the categorization within the training, testing, and validation directories\n",
    "for i in range(5):\n",
    "    entropy_group_dir_train = path.join(training_directory, 'entropy_groups', f'entropy_group_{i}')\n",
    "    entropy_group_dir_test = path.join(testing_directory, 'entropy_groups', f'entropy_group_{i}')\n",
    "    entropy_group_dir_valid = path.join(valid_directory, 'entropy_groups', f'entropy_group_{i}')\n",
    "    if path.exists(entropy_group_dir_train):\n",
    "        categorize_images(entropy_group_dir_train)\n",
    "    if path.exists(entropy_group_dir_test):\n",
    "        categorize_images(entropy_group_dir_test)\n",
    "    if path.exists(entropy_group_dir_valid):\n",
    "        categorize_images(entropy_group_dir_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enhanse images according to brigtness-entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-22T16:27:19.753151Z",
     "start_time": "2024-03-22T16:26:56.258202Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to increase brightness and contrast\n",
    "def adjust_brightness_contrast(input_image_path, output_image_path, brightness=1, contrast=1):\n",
    "    # Open the image file\n",
    "    img = Image.open(input_image_path)\n",
    "\n",
    "    # Enhance brightness\n",
    "    enhancer = ImageEnhance.Brightness(img)\n",
    "    img = enhancer.enhance(brightness)\n",
    "\n",
    "    # Enhance contrast\n",
    "    enhancer = ImageEnhance.Contrast(img)\n",
    "    img = enhancer.enhance(contrast)\n",
    "\n",
    "    # Save the edited image\n",
    "    img.save(output_image_path)\n",
    "    img.close()\n",
    "\n",
    "# Define the brightness and contrast values for each entropy group folder\n",
    "entropy_group_values = { #checked and cleared\n",
    "    'entropy_group_0': { \n",
    "        \"dark_brightness\": (1.25, 1.5), #checked and cleared\n",
    "        \"extra_low_brightness\": (1, 1.25), #checked and cleared\n",
    "        \"high_brightness\": (0.75, 1.5), #does not exist\n",
    "        \"low_brightness\": (1, 1.25), #does not exist\n",
    "        \"medium_brightness\": (1, 1), #does not exist\n",
    "        \"very_dark_brightness\": (2.25, 2.5), #checked and cleared\n",
    "        \"very_low_brightness\": (1, 0.75)  \n",
    "    },\n",
    "\n",
    "    'entropy_group_1': { \n",
    "        \"dark_brightness\": (1.75, 1.75), #checked and cleared\n",
    "        \"extra_low_brightness\": (1.3, 0.75), #checked and cleared\n",
    "        \"high_brightness\": (0.75, 1.5), #does not exist\n",
    "        \"low_brightness\": (0.75, 1.5), #checked and cleared\n",
    "        \"medium_brightness\": (1, 1), #does not exist\n",
    "        \"very_dark_brightness\": (2.25, 2.5), #still to dark, cannot justify increase due to fidelity loss\n",
    "        \"very_low_brightness\": (1, 0.75) #checked and cleared\n",
    "    },\n",
    "\n",
    "    'entropy_group_2': { #checked and cleared\n",
    "        \"dark_brightness\": (2, 2), #checked and cleared\n",
    "        \"extra_low_brightness\": (1.25, 1.25), #checked and cleared\n",
    "        \"high_brightness\": (0.75, 1.5), #checked and cleared\n",
    "        \"low_brightness\": (1, 1.25), #checked and cleared\n",
    "        \"medium_brightness\": (1, 1), #checked and cleared\n",
    "        \"very_dark_brightness\": (2.25, 2.5), #checked and cleared\n",
    "        \"very_low_brightness\": (1, 1.25) #checked and cleared\n",
    "    },\n",
    "\n",
    "    'entropy_group_3': { #checked and cleared\n",
    "        \"dark_brightness\": (2, 2), #checked and cleared\n",
    "        \"extra_low_brightness\": (1.5, 1.25), #checked and cleared\n",
    "        \"high_brightness\": (0.75, 1.75), #checked and cleared\n",
    "        \"low_brightness\": (1, 1.25), #checked and cleared\n",
    "        \"medium_brightness\": (1, 1), #checked and cleared\n",
    "        \"very_dark_brightness\": (3, 2),  #still to dark, cannot justify increase due to fidelity loss\n",
    "        \"very_low_brightness\": (1, 1.25) #checked and cleared\n",
    "    },\n",
    "\n",
    "    'entropy_group_4': { \n",
    "        \"dark_brightness\": (2, 2),  #checked and cleared\n",
    "        \"extra_low_brightness\": (1.5, 1.25), #checked and cleared\n",
    "        \"high_brightness\": (0.75, 2), #checked and cleared\n",
    "        \"low_brightness\": (1, 1.25), #checked and cleared\n",
    "        \"medium_brightness\": (1, 1.15), #checked and cleared\n",
    "        \"very_dark_brightness\": (2.25, 2.5), #does not exist\n",
    "        \"very_low_brightness\": (1, 1.25) #checked and cleared\n",
    "    }\n",
    "}\n",
    "\n",
    "# Define a list of the master directories\n",
    "master_directories = [training_directory, testing_directory, valid_directory]\n",
    "\n",
    "# Loop through each master directory\n",
    "for master_directory in master_directories:\n",
    "    # Loop through each entropy group folder in the dictionary\n",
    "    for entropy_group_folder, folder_values in entropy_group_values.items():\n",
    "        # Define the directory path for the current entropy group folder\n",
    "        dir_path = path.join(master_directory, 'entropy_groups', entropy_group_folder)\n",
    "\n",
    "        # Loop through each brightness folder in the dictionary\n",
    "        for folder, (brightness_value, contrast_value) in folder_values.items():\n",
    "            # Define the input and output paths\n",
    "            input_dir_path = path.join(dir_path, folder)\n",
    "            output_dir_path = path.join(input_dir_path, 'edited_images')\n",
    "\n",
    "            # Remove the directory if it exists\n",
    "            if path.exists(output_dir_path):\n",
    "                shutil.rmtree(output_dir_path)\n",
    "\n",
    "            # Create a new directory for the edited images\n",
    "            makedirs(output_dir_path, exist_ok=True)\n",
    "\n",
    "            # Loop through each file in the directory\n",
    "            for filename in listdir(input_dir_path):\n",
    "                # Check if the file is an image\n",
    "                if filename.endswith('.jpg') or filename.endswith('.png'):\n",
    "                    # Define the input and output paths\n",
    "                    input_image_path = path.join(input_dir_path, filename)\n",
    "                    output_image_path = path.join(output_dir_path, filename)\n",
    "\n",
    "                    # Adjust brightness and contrast of the image\n",
    "                    adjust_brightness_contrast(input_image_path, output_image_path, brightness=brightness_value, contrast=contrast_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "move files to master location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-22T16:27:30.212752Z",
     "start_time": "2024-03-22T16:27:28.363116Z"
    }
   },
   "outputs": [],
   "source": [
    "def move_augmented_images(master_directories, output_directory):\n",
    "    # Define the new directories\n",
    "    new_directories = ['new_train', 'new_test', 'new_valid']\n",
    "\n",
    "    # Loop through each master directory\n",
    "    for master_directory, new_directory in zip(master_directories, new_directories):\n",
    "        # Define the new directory path\n",
    "        new_dir_path = path.join(output_directory, new_directory)\n",
    "\n",
    "        # Create the new directory if it doesn't exist\n",
    "        if not path.exists(new_dir_path):\n",
    "            makedirs(new_dir_path)\n",
    "\n",
    "        # Loop through each entropy group folder in the dictionary\n",
    "        for entropy_group_folder in entropy_group_values.keys():\n",
    "            # Define the directory path for the current entropy group folder\n",
    "            dir_path = path.join(master_directory, 'entropy_groups', entropy_group_folder)\n",
    "\n",
    "            # Loop through each brightness folder in the dictionary\n",
    "            for folder in entropy_group_values[entropy_group_folder].keys():\n",
    "                # Define the input and output paths\n",
    "                input_dir_path = path.join(dir_path, folder, 'edited_images')\n",
    "                output_dir_path = path.join(new_dir_path, entropy_group_folder, folder)\n",
    "\n",
    "                # Create the output directory if it doesn't exist\n",
    "                if not path.exists(output_dir_path):\n",
    "                    makedirs(output_dir_path)\n",
    "\n",
    "                # Loop through each file in the directory\n",
    "                for filename in listdir(input_dir_path):\n",
    "                    # Check if the file is an image\n",
    "                    if filename.endswith('.jpg') or filename.endswith('.png'):\n",
    "                        # Define the input and output paths\n",
    "                        input_image_path = path.join(input_dir_path, filename)\n",
    "                        output_image_path = path.join(output_dir_path, filename)\n",
    "\n",
    "                        # Move the image to the new directory\n",
    "                        shutil.copy(input_image_path, output_image_path)\n",
    "\n",
    "        # Copy the _annotations.csv file to the new directory\n",
    "        annotations_file_path = path.join(master_directory, '_annotations.csv')\n",
    "        if path.exists(annotations_file_path):\n",
    "            shutil.copy(annotations_file_path, new_dir_path)\n",
    "\n",
    "def move_images_to_top(master_directories, output_directory):\n",
    "    # Define the new directories\n",
    "    new_directories = ['new_train', 'new_test', 'new_valid']\n",
    "\n",
    "    # Loop through each master directory\n",
    "    for master_directory, new_directory in zip(master_directories, new_directories):\n",
    "        # Define the new directory path\n",
    "        new_dir_path = path.join(output_directory, new_directory)\n",
    "\n",
    "        # Loop through each entropy group folder in the dictionary\n",
    "        for entropy_group_folder in entropy_group_values.keys():\n",
    "            # Define the directory path for the current entropy group folder\n",
    "            dir_path = path.join(new_dir_path, entropy_group_folder)\n",
    "\n",
    "            # Find all images in the entropy group folder and its subfolders\n",
    "            images = glob.glob(path.join(dir_path, '**', '*.jpg'), recursive=True)\n",
    "            images += glob.glob(path.join(dir_path, '**', '*.png'), recursive=True)\n",
    "\n",
    "            # Move each image to the top level of the new directory\n",
    "            for image_path in images:\n",
    "                shutil.move(image_path, new_dir_path)\n",
    "\n",
    "            # Delete the entropy group folder\n",
    "            shutil.rmtree(dir_path)\n",
    "\n",
    "move_augmented_images(master_directories, output_directory)\n",
    "move_images_to_top(master_directories, output_directory)\n",
    "training_directory = path.join(output_directory, 'new_train')\n",
    "testing_directory = path.join(output_directory, 'new_test')\n",
    "valid_directory = path.join(output_directory, 'new_valid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "resize images and update CSV and fill in blanks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-22T16:29:11.330515Z",
     "start_time": "2024-03-22T16:29:02.601208Z"
    }
   },
   "outputs": [],
   "source": [
    "def resize_images_and_boxes(directory, size=(256, 256)):\n",
    "    # Define the column names\n",
    "    column_names = ['filename', 'x1', 'y1', 'x2', 'y2', 'label']\n",
    "\n",
    "    # Load the annotations file\n",
    "    df = pd.read_csv(path.join(directory, '_annotations.csv'), names=column_names)\n",
    "\n",
    "    # Replace 'humerus' with 'humerus fracture'\n",
    "    df['label'] = df['label'].replace('humerus', 'humerus fracture')\n",
    "\n",
    "    # Loop through each unique filename in the DataFrame\n",
    "    for filename in df['filename'].unique():\n",
    "        # Load the image\n",
    "        image = Image.open(path.join(directory, filename))\n",
    "\n",
    "        # Get the original image size\n",
    "        original_size = image.size\n",
    "\n",
    "        # Resize the image\n",
    "        image = image.resize(size)\n",
    "        image.save(path.join(directory, filename))\n",
    "\n",
    "        # Adjust the bounding boxes\n",
    "        df.loc[df['filename'] == filename, ['x1', 'x2']] = (df.loc[df['filename'] == filename, ['x1', 'x2']] * size[0] / original_size[0]).round()\n",
    "        df.loc[df['filename'] == filename, ['y1', 'y2']] = (df.loc[df['filename'] == filename, ['y1', 'y2']] * size[1] / original_size[1]).round()\n",
    "\n",
    "    # Check for images in the directory that are not in the DataFrame\n",
    "    for image_file in glob.glob(path.join(directory, '*.jpg')):\n",
    "        filename = path.basename(image_file)\n",
    "        if filename not in df['filename'].values:\n",
    "            # Add the missing image to the DataFrame with the label 'None'\n",
    "            df = df._append({'filename': filename, 'label': 'None'}, ignore_index=True)\n",
    "\n",
    "    # Save the adjusted annotations\n",
    "    df.to_csv(path.join(directory, '_annotations.csv'), index=False)\n",
    "\n",
    "resize_images_and_boxes(training_directory)\n",
    "resize_images_and_boxes(testing_directory)\n",
    "resize_images_and_boxes(valid_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create a group of inverted images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-23T10:03:46.546363Z",
     "start_time": "2024-03-23T10:03:46.470522Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'listdir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[30], line 27\u001B[0m\n\u001B[1;32m     24\u001B[0m             inverted_image\u001B[38;5;241m.\u001B[39msave(output_image_path)\n\u001B[1;32m     26\u001B[0m \u001B[38;5;66;03m# Use the function to copy and invert the images\u001B[39;00m\n\u001B[0;32m---> 27\u001B[0m \u001B[43mcopy_and_invert_images\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43mr\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mretinanet_data_format/new_training\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43mr\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mretinanet_data_format/new_training_inverted\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     28\u001B[0m copy_and_invert_images(\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mretinanet_data_format/new_testing\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mretinanet_data_format/new_testing_inverted\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     29\u001B[0m copy_and_invert_images(\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mretinanet_data_format/new_valid\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mretinanet_data_format/new_valid_inverted\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "Cell \u001B[0;32mIn[30], line 10\u001B[0m, in \u001B[0;36mcopy_and_invert_images\u001B[0;34m(input_directory, output_directory)\u001B[0m\n\u001B[1;32m      4\u001B[0m     makedirs(output_directory)\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# Copy the _annotations.csv file to the new directory\u001B[39;00m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m#shutil.copy(os.path.join(input_directory, '_annotations.csv'), output_directory)\u001B[39;00m\n\u001B[1;32m      8\u001B[0m \n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m# Loop through each file in the directory\u001B[39;00m\n\u001B[0;32m---> 10\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m filename \u001B[38;5;129;01min\u001B[39;00m \u001B[43mlistdir\u001B[49m(input_directory):\n\u001B[1;32m     11\u001B[0m     \u001B[38;5;66;03m# Check if the file is an image\u001B[39;00m\n\u001B[1;32m     12\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m filename\u001B[38;5;241m.\u001B[39mendswith(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.jpg\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m filename\u001B[38;5;241m.\u001B[39mendswith(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.png\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[1;32m     13\u001B[0m         \u001B[38;5;66;03m# Define the input and output paths\u001B[39;00m\n\u001B[1;32m     14\u001B[0m         input_image_path \u001B[38;5;241m=\u001B[39m path\u001B[38;5;241m.\u001B[39mjoin(input_directory, filename)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'listdir' is not defined"
     ]
    }
   ],
   "source": [
    "def copy_and_invert_images(input_directory, output_directory):\n",
    "    # Create the output directory if it doesn't exist\n",
    "    if not path.exists(output_directory):\n",
    "        makedirs(output_directory)\n",
    "\n",
    "    # Copy the _annotations.csv file to the new directory\n",
    "    #shutil.copy(os.path.join(input_directory, '_annotations.csv'), output_directory)\n",
    "\n",
    "    # Loop through each file in the directory\n",
    "    for filename in listdir(input_directory):\n",
    "        # Check if the file is an image\n",
    "        if filename.endswith('.jpg') or filename.endswith('.png'):\n",
    "            # Define the input and output paths\n",
    "            input_image_path = path.join(input_directory, filename)\n",
    "            output_image_path = path.join(output_directory, filename)\n",
    "\n",
    "            # Load the image\n",
    "            image = Image.open(input_image_path)\n",
    "\n",
    "            # Invert the image\n",
    "            inverted_image = ImageOps.invert(image)\n",
    "\n",
    "            # Save the inverted image\n",
    "            inverted_image.save(output_image_path)\n",
    "\n",
    "# Use the function to copy and invert the images\n",
    "copy_and_invert_images(r'retinanet_data_format/new_train', r'retinanet_data_format/new_train_inverted')\n",
    "copy_and_invert_images(r'retinanet_data_format/new_test', r'retinanet_data_format/new_test_inverted')\n",
    "copy_and_invert_images(r'retinanet_data_format/new_valid', r'retinanet_data_format/new_valid_inverted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create an overlayed image directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-22T16:37:39.531575Z",
     "start_time": "2024-03-22T16:37:22.855421Z"
    }
   },
   "outputs": [],
   "source": [
    "def overlay_images(input_directory1, input_directory2, output_directory):\n",
    "    # Create the output directory if it doesn't exist\n",
    "    if not path.exists(output_directory):\n",
    "        makedirs(output_directory)\n",
    "\n",
    "    # Copy the _annotations.csv file from the first input directory to the new directory\n",
    "    shutil.copy(path.join(input_directory1, '_annotations.csv'), output_directory)\n",
    "\n",
    "    # Loop through each file in the first input directory\n",
    "    for filename in listdir(input_directory1):\n",
    "        # Check if the file is an image\n",
    "        if filename.endswith('.jpg') or filename.endswith('.png'):\n",
    "            # Define the input paths\n",
    "            input_image_path1 = path.join(input_directory1, filename)\n",
    "            input_image_path2 = path.join(input_directory2, filename)\n",
    "\n",
    "            # Load the images\n",
    "            image1 = Image.open(input_image_path1).convert('RGB')\n",
    "            image2 = Image.open(input_image_path2).convert('RGB')\n",
    "\n",
    "            # Overlay the images\n",
    "            overlayed_image = ImageChops.darker(image1, image2)\n",
    "\n",
    "            # Save the overlayed image\n",
    "            overlayed_image.save(path.join(output_directory, filename))\n",
    "\n",
    "# Use the function to overlay the images\n",
    "\n",
    "overlay_images(r'retinanet_data_format/new_train', r'retinanet_data_format/new_train_inverted', r'retinanet_data_format/new_train_overlayed')\n",
    "overlay_images(r'retinanet_data_format/new_test', r'retinanet_data_format/new_test_inverted', r'retinanet_data_format/new_test_overlayed')\n",
    "overlay_images(r'retinanet_data_format/new_valid', r'retinanet_data_format/new_valid_inverted', r'retinanet_data_format/new_valid_overlayed')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove absolute black (WIP)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## move the files to a binary categorised data structure"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def categorise_images(a_dataset_name):\n",
    "    #creates a directory structure  like: images/\n",
    "    #                                       class0/*.jpg\n",
    "    #                                       class1/*.jpg\n",
    "    #and copies the images into either leaf directory depending on their class label\n",
    "    #this puts the data into a format that keras can load into a dataset\n",
    "    \n",
    "    CLASS_DIRECTORY_NAMES = ('class_fracture','class_no_fracture')\n",
    "    DATASET_DIRECTORY = 'retinanet_data_format'\n",
    "    TOP_DIRECTORY_CLASSIFIFICATIONS = path.join(DATASET_DIRECTORY ,'categorised_images')\n",
    "\n",
    "    fracture_directory = path.join(TOP_DIRECTORY_CLASSIFIFICATIONS, a_dataset_name,CLASS_DIRECTORY_NAMES[0])\n",
    "    print('fracture_directory', fracture_directory)\n",
    "    no_fracture_directory = path.join(TOP_DIRECTORY_CLASSIFIFICATIONS,a_dataset_name,CLASS_DIRECTORY_NAMES[-1])\n",
    "    print('no_fracture_directory', no_fracture_directory)\n",
    "\n",
    "    if path.exists(fracture_directory) == False:  \n",
    "        makedirs(fracture_directory) \n",
    "    if path.exists(no_fracture_directory) == False:  \n",
    "        makedirs(no_fracture_directory) \n",
    "\n",
    "    #read labels into pandas df\n",
    "    csv_filepath = path.join('dataset', a_dataset_name, 'labels/labels.csv')\n",
    "    print('csv_filepath', csv_filepath)\n",
    "    df = pd.read_csv(path.join(csv_filepath))\n",
    "    df = df.set_index('filename')\n",
    "    print('df', df.head())\n",
    "\n",
    "    #walk through files and copy file to relevant directory\n",
    "    images_dir = path.join(DATASET_DIRECTORY, 'new_' + a_dataset_name + '_overlayed')\n",
    "    print('images_dir', images_dir)\n",
    "    filenames = []\n",
    "    for (dirpath, dirnames, filenames) in walk(images_dir):\n",
    "        filenames.extend(filenames)\n",
    "    print('filenames', filenames[:10])\n",
    "    print('len(filenames)', len(filenames))\n",
    "    for filename in filenames:\n",
    "        #ignore mac os system file\n",
    "        if filename.endswith('.jpg') == False:\n",
    "            pass\n",
    "        elif df.loc[filename]['target'] == 1:\n",
    "            destination = path.join(fracture_directory, filename)\n",
    "        else:\n",
    "            destination = path.join(no_fracture_directory, filename)\n",
    "        source = path.join(images_dir, filename)\n",
    "        copyfile(source, destination)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-24T07:25:18.251660Z",
     "start_time": "2024-03-24T07:25:18.243660Z"
    }
   },
   "execution_count": 52
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fracture_directory retinanet_data_format/categorised_images/train/class_fracture\n",
      "no_fracture_directory retinanet_data_format/categorised_images/train/class_no_fracture\n",
      "csv_filepath dataset/train/labels/labels.csv\n",
      "df                                                     target\n",
      "filename                                                  \n",
      "image1_753_png.rf.611c51510c1794dbf76a673076783...       1\n",
      "image1_888_png.rf.614a3cdebe5fb086b60ccef277a97...       1\n",
      "image1_208_png.rf.61714b3d44ca4a1981225a2910543...       1\n",
      "image1_766_png.rf.6185692944249a1226ae9ed74c329...       0\n",
      "image1_68_png.rf.60970c5e5b05495c1d0d4950a95d57...       0\n",
      "images_dir retinanet_data_format/new_train_overlayed\n",
      "filenames ['image1_273_png.rf.f022d65b5a20cc363d04891eabb54357.jpg', 'image1_3519_png.rf.d216fecf8d40b460c9937c466a46ad55.jpg', 'image2_535_png.rf.cf4d40b370c59fa5a126e2120d33ea6c.jpg', 'image1_459_png.rf.002fc7bef16dddfc7dd77a8adb9a6a3d.jpg', 'image1_3466_png.rf.90386a2ceaa6ba28df75e7aa1abbd854.jpg', 'image1_201_png.rf.82d0c3154f2c8907adb44d2c3c69543c.jpg', 'image1_4840_png.rf.39ac20932ca5db1cbfc25ff77df55bd4.jpg', 'image1_473_png.rf.c4306c3b947956ccd4f81d72fbaa5041.jpg', 'image1_850_png.rf.5cc00998aff4371a8b8c67c73ed038ed.jpg', 'image1_437_png.rf.2c5ee66c74ffd08bd4af0cd58a3fc8ab.jpg']\n",
      "len(filenames) 7264\n",
      "fracture_directory retinanet_data_format/categorised_images/valid/class_fracture\n",
      "no_fracture_directory retinanet_data_format/categorised_images/valid/class_no_fracture\n",
      "csv_filepath dataset/valid/labels/labels.csv\n",
      "df                                                     target\n",
      "filename                                                  \n",
      "image1_162_png.rf.0132591669e00deaf033b6d409e4d...       0\n",
      "image1_7058_png.rf.029c63a7047dedf16c6d0ce01533...       1\n",
      "image1_1096_png.rf.0201a3553b8b76991d514ced849d...       0\n",
      "image1_3613_png.rf.04279ec0c858ecc936ec0199d6e8...       1\n",
      "image1_382_png.rf.051ac683451d7606ad307fdeddfb4...       0\n",
      "images_dir retinanet_data_format/new_valid_overlayed\n",
      "filenames ['image1_96_png.rf.84e1d4c27219711b3bab09d6f54745b0.jpg', 'image1_1134_png.rf.d654c836fa7d0df0490cc25e4b8b841d.jpg', 'image1_264_png.rf.386e1a0bf8e66735ae91a5bcfc01aed5.jpg', 'image1_4160_png.rf.da4b4d52bb392c4587f10a38b8818fb5.jpg', 'image1_277_png.rf.6bec675762045dd3e65c3b265789d9d9.jpg', 'image1_340_png.rf.38e0fc7764336e7a7e3c440aea0e2daa.jpg', 'image2_345_png.rf.2a31e2cc93af547720719038f3207b42.jpg', 'image1_3619_png.rf.1adbb7f2ad4dfbd1e89dc200283e8fb3.jpg', 'image1_1095_png.rf.528793dae32e5d8d6aca1d9bbc7a4511.jpg', 'image1_548_png.rf.042f9ef1d41fe9e8cfc84c43a6d33bed.jpg']\n",
      "len(filenames) 698\n",
      "fracture_directory retinanet_data_format/categorised_images/test/class_fracture\n",
      "no_fracture_directory retinanet_data_format/categorised_images/test/class_no_fracture\n",
      "csv_filepath dataset/test/labels/labels.csv\n",
      "df                                                     target\n",
      "filename                                                  \n",
      "image2_199_png.rf.111ecddb2bdc3542d7f953385d1bb...       1\n",
      "image1_3073_png.rf.241285b7d595353800440e6d2b9b...       1\n",
      "image1_268_png.rf.00406f976b1bd6c978e828d2c5085...       0\n",
      "image1_1981_png.rf.05106dc3354a8d2c4292cacd2813...       1\n",
      "image1_540_png.rf.0e3e26d67bb471fb263c983cdf758...       0\n",
      "images_dir retinanet_data_format/new_test_overlayed\n",
      "filenames ['image1_704_png.rf.e3668d404753b933e1fd9500e697ca8c.jpg', 'image1_1326_png.rf.6019c7e15df84d9d6286932d03e687b6.jpg', 'image2_1718_png.rf.3320525501293fd17921dbca2ec70ebf.jpg', 'image2_812_png.rf.cfcd3189eb853066fc4819ad746775a6.jpg', 'image1_6478_png.rf.eee144dfa7e8712592f7d8b10523e2fa.jpg', 'image1_860_png.rf.37fd7f57d0b9ec38eafafceacdbd4f4c.jpg', 'image1_1044_png.rf.1e17d3a8637036ef4b3e1c5d0b88011f.jpg', 'image1_1392_png.rf.692d8594c2ce39483af8c3c4fee91646.jpg', 'image1_1070_png.rf.d01735eab2c774146e40eef6c8e7b661.jpg', 'image1_182_png.rf.c9539857a1608ea4850d37195ff767ce.jpg']\n",
      "len(filenames) 340\n"
     ]
    }
   ],
   "source": [
    "DATASET_NAMES = ('train','valid','test')\n",
    "\n",
    "for dataset_name in DATASET_NAMES:\n",
    "    categorise_images(dataset_name)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-24T07:25:21.970842Z",
     "start_time": "2024-03-24T07:25:19.653708Z"
    }
   },
   "execution_count": 53
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last modified:  24/03/2024 07:36:13\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "print(\"Last modified: \", datetime.datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\") + \"\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-24T07:36:13.060458Z",
     "start_time": "2024-03-24T07:36:13.055582Z"
    }
   },
   "execution_count": 54
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
